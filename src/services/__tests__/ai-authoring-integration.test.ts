import { describe, it, expect, vi, beforeEach, afterEach } from 'vitest'
import { AIAuthoringService } from '../ai-authoring'
import { AutoContinueService } from '../auto-continue'
import { ShortformEvaluatorService } from '../shortform-evaluator'
import { errorHandler } from '../error-handler'
import type { 
  KataGenerationRequest, 
  GeneratedKataContent, 
  Kata, 
  KataFilters,
  MultiQuestionConfig,
  ShortformQuestion
} from '@/types'

// Mock the dependencies
vi.mock('../ai-config')
vi.mock('../prompt-engine')
vi.mock('../response-parser')
vi.mock('../content-validator')
vi.mock('../file-generator')
vi.mock('../generation-history')
vi.mock('../ai-authoring-error-handler')
vi.mock('../error-handler')

// Mock AutoContinueService
vi.mock('../auto-continue', () => ({
  AutoContinueService: {
    getInstance: vi.fn(() => ({
      getRandomKata: vi.fn(),
      getRandomKataFromFiltered: vi.fn(),
      createNotification: vi.fn(),
      shouldTrigger: vi.fn()
    }))
  }
}))

// Mock ShortformEvaluatorService
vi.mock('../shortform-evaluator', () => ({
  ShortformEvaluatorService: {
    getInstance: vi.fn(() => ({
      evaluateMultiQuestionSubmission: vi.fn(),
      evaluateSubmission: vi.fn(),
      validateSubmission: vi.fn()
    }))
  }
}))

describe('AI Authoring Integration Tests', () => {
  let aiAuthoringService: AIAuthoringService
  let autoContinueService: AutoContinueService
  let shortformEvaluator: ShortformEvaluatorService

  beforeEach(() => {
    vi.clearAllMocks()
    
    // Reset singletons
    ;(AIAuthoringService as any).instance = undefined
    
    aiAuthoringService = AIAuthoringService.getInstance()
    autoContinueService = AutoContinueService.getInstance()
    shortformEvaluator = ShortformEvaluatorService.getInstance()
  })

  afterEach(() => {
    vi.restoreAllMocks()
  })

  describe('AutoContinueService Integration', () => {
    it('should work with generated katas in auto-continue flow', async () => {
      // Mock a generated kata
      const generatedKata: Kata = {
        slug: 'ai-generated-kata',
        title: 'AI Generated Kata',
        language: 'py',
        difficulty: 'easy',
        type: 'code',
        tags: ['ai-generated'],
        path: '/path/to/kata'
      }

      const availableKatas: Kata[] = [
        generatedKata,
        {
          slug: 'existing-kata',
          title: 'Existing Kata',
          language: 'py',
          difficulty: 'easy',
          type: 'code',
          tags: ['manual'],
          path: '/path/to/existing'
        }
      ]

      const filters: KataFilters = {
        difficulty: ['easy'],
        language: ['py'],
        type: ['code']
      }

      // Test that auto-continue can select generated katas
      const randomKata = autoContinueService.getRandomKata(
        availableKatas[1], // current kata
        availableKatas,
        filters
      )

      expect(randomKata).toBeTruthy()
      expect([generatedKata.slug]).toContain(randomKata?.slug)
    })

    it('should handle generated katas with tags in filtering', () => {
      const generatedKata: Kata = {
        slug: 'ai-generated-kata',
        title: 'AI Generated Kata',
        language: 'js',
        difficulty: 'medium',
        type: 'code',
        tags: ['algorithms', 'ai-generated'],
        path: '/path/to/kata'
      }

      const availableKatas: Kata[] = [generatedKata]
      const filters: KataFilters = {
        tags: ['algorithms']
      }

      const randomKata = autoContinueService.getRandomKataFromFiltered(
        availableKatas,
        filters
      )

      expect(randomKata).toEqual(generatedKata)
    })

    it('should create proper notifications for generated katas', () => {
      const fromKata: Kata = {
        slug: 'manual-kata',
        title: 'Manual Kata',
        language: 'py',
        difficulty: 'easy',
        type: 'code',
        tags: [],
        path: '/path/to/manual'
      }

      const toKata: Kata = {
        slug: 'ai-generated-kata',
        title: 'AI Generated Advanced Problem',
        language: 'py',
        difficulty: 'easy',
        type: 'code',
        tags: ['ai-generated'],
        path: '/path/to/generated'
      }

      const notification = autoContinueService.createNotification(fromKata, toKata)

      expect(notification.message).toContain('Manual Kata')
      expect(notification.message).toContain('AI Generated Advanced Problem')
      expect(notification.fromKata).toBe(fromKata.slug)
      expect(notification.toKata).toBe(toKata.slug)
      expect(notification.timestamp).toBeInstanceOf(Date)
    })
  })

  describe('ShortformEvaluatorService Integration', () => {
    it('should evaluate generated multi-question katas', async () => {
      const multiQuestionConfig: MultiQuestionConfig = {
        title: 'AI Generated Assessment',
        description: 'Generated by AI authoring system',
        questions: [
          {
            id: 'q1',
            type: 'multiple-choice',
            question: 'What is the time complexity of binary search?',
            options: [
              { id: 'a', text: 'O(n)' },
              { id: 'b', text: 'O(log n)' },
              { id: 'c', text: 'O(nÂ²)' }
            ],
            correctAnswers: ['b'],
            points: 10
          },
          {
            id: 'q2',
            type: 'shortform',
            question: 'Explain the concept of recursion',
            expectedAnswer: 'A function calling itself',
            acceptableAnswers: ['function calls itself', 'self-calling function'],
            points: 15
          }
        ],
        passingScore: 70,
        allowReview: true,
        showProgressBar: true
      }

      const submission = {
        kataType: 'multi-question' as const,
        answers: {
          'q1': ['b'],
          'q2': 'A function calling itself with a base case'
        },
        multiQuestionConfig
      }

      // Mock the evaluateMultiQuestionSubmission method
      const mockResult = {
        success: true,
        output: 'Assessment completed successfully',
        errors: '',
        testResults: [
          {
            name: 'Question: What is the time complexity of binary search?...',
            passed: true,
            message: 'Correct!',
            expected: 'b',
            actual: ['b']
          },
          {
            name: 'Question: Explain the concept of recursion...',
            passed: true,
            message: 'Correct!',
            expected: 'A function calling itself',
            actual: 'A function calling itself with a base case'
          }
        ],
        score: 100,
        duration: 150
      }

      vi.spyOn(shortformEvaluator, 'evaluateMultiQuestionSubmission')
        .mockResolvedValue(mockResult)

      const result = await shortformEvaluator.evaluateMultiQuestionSubmission(submission)

      expect(result.success).toBe(true)
      expect(result.score).toBe(100)
      expect(result.testResults).toHaveLength(2)
      expect(shortformEvaluator.evaluateMultiQuestionSubmission).toHaveBeenCalledWith(submission)
    })

    it('should handle generated shortform katas with various question types', async () => {
      const shortformConfig = {
        question: 'What is the capital of France?',
        expectedAnswer: 'Paris',
        acceptableAnswers: ['paris', 'Paris', 'PARIS'],
        caseSensitive: false,
        maxLength: 100
      }

      const submission = {
        kataType: 'shortform' as const,
        answer: 'Paris',
        shortformConfig
      }

      const mockResult = {
        success: true,
        output: 'Correct! Your answer matches the expected response.',
        errors: '',
        testResults: [{
          name: 'Answer Validation',
          passed: true,
          message: 'Correct! Your answer matches the expected response.',
          expected: 'Paris',
          actual: 'Paris'
        }],
        score: 100,
        duration: 50
      }

      vi.spyOn(shortformEvaluator, 'evaluateSubmission')
        .mockResolvedValue(mockResult)

      const result = await shortformEvaluator.evaluateSubmission(submission)

      expect(result.success).toBe(true)
      expect(result.score).toBe(100)
      expect(shortformEvaluator.evaluateSubmission).toHaveBeenCalledWith(submission)
    })

    it('should validate generated kata submissions properly', () => {
      const validSubmission = {
        kataType: 'one-liner' as const,
        answer: 'Quick answer',
        oneLinerConfig: {
          question: 'What is 2+2?',
          expectedAnswer: '4',
          acceptableAnswers: ['four', 'Four'],
          caseSensitive: false
        }
      }

      const validation = shortformEvaluator.validateSubmission(validSubmission)

      expect(validation.isValid).toBe(true)
      expect(validation.errors).toHaveLength(0)
    })
  })

  describe('Error Handler Integration', () => {
    it('should handle AI authoring errors through global error handler', async () => {
      const mockError = new Error('API rate limit exceeded')
      const context = { operation: 'generateKata', request: {} }

      vi.spyOn(errorHandler, 'handleAIServiceError')
        .mockReturnValue({
          type: 'AI_SERVICE_ERROR',
          message: 'AI service rate limit exceeded',
          details: 'API rate limit exceeded',
          timestamp: new Date(),
          recoverable: true,
          context
        })

      const result = errorHandler.handleAIServiceError(mockError as any, context)

      expect(result.type).toBe('AI_SERVICE_ERROR')
      expect(result.recoverable).toBe(true)
      expect(errorHandler.handleAIServiceError).toHaveBeenCalledWith(mockError, context)
    })

    it('should handle file system errors during kata saving', () => {
      const fileSystemError = {
        code: 'ENOENT',
        path: '/path/to/kata',
        message: 'File not found'
      }

      vi.spyOn(errorHandler, 'handleFileSystemError')
        .mockReturnValue({
          type: 'FILE_SYSTEM_ERROR',
          message: 'File or directory not found: /path/to/kata',
          details: 'File not found',
          timestamp: new Date(),
          recoverable: true,
          context: { code: 'ENOENT', path: '/path/to/kata' }
        })

      const result = errorHandler.handleFileSystemError(fileSystemError as any)

      expect(result.type).toBe('FILE_SYSTEM_ERROR')
      expect(result.message).toContain('/path/to/kata')
      expect(errorHandler.handleFileSystemError).toHaveBeenCalledWith(fileSystemError)
    })

    it('should provide recovery options for AI authoring failures', () => {
      const networkError = new Error('Network connection failed')
      const recoveryOptions = {
        retry: vi.fn(),
        fallback: vi.fn(),
        ignore: vi.fn()
      }

      vi.spyOn(errorHandler, 'handleNetworkError')
        .mockReturnValue({
          type: 'NETWORK_ERROR',
          message: 'Network connection failed',
          details: 'Network connection failed',
          timestamp: new Date(),
          recoverable: true,
          context: {}
        })

      const result = errorHandler.handleNetworkError(networkError, {}, recoveryOptions)

      expect(result.type).toBe('NETWORK_ERROR')
      expect(result.recoverable).toBe(true)
      expect(errorHandler.handleNetworkError).toHaveBeenCalledWith(
        networkError, 
        {}, 
        recoveryOptions
      )
    })
  })

  describe('Settings Integration', () => {
    it('should integrate AI configuration with user settings', async () => {
      // This test would verify that AI configuration is properly integrated
      // with the existing settings system, but since we're mocking the services,
      // we'll just verify the integration points exist

      expect(aiAuthoringService).toBeDefined()
      expect(typeof aiAuthoringService.generateKata).toBe('function')
      expect(typeof aiAuthoringService.generateVariation).toBe('function')
      expect(typeof aiAuthoringService.saveGeneratedKata).toBe('function')
    })
  })

  describe('Multi-Question Panel Integration', () => {
    it('should generate compatible multi-question configurations', () => {
      // Test that generated multi-question katas work with existing MultiQuestionPanel
      const generatedConfig: MultiQuestionConfig = {
        title: 'Generated Assessment',
        description: 'AI-generated multi-question assessment',
        questions: [
          {
            id: 'gen-q1',
            type: 'code',
            question: 'Implement a function to reverse a string',
            language: 'python',
            starterCode: 'def reverse_string(s):\n    pass',
            points: 20
          },
          {
            id: 'gen-q2',
            type: 'explanation',
            question: 'Explain the time complexity of your solution',
            minWords: 50,
            points: 10
          }
        ],
        passingScore: 75,
        allowReview: true,
        showProgressBar: true
      }

      // Verify the structure matches what MultiQuestionPanel expects
      expect(generatedConfig.title).toBeDefined()
      expect(generatedConfig.questions).toBeInstanceOf(Array)
      expect(generatedConfig.questions.length).toBeGreaterThan(0)
      expect(generatedConfig.passingScore).toBeTypeOf('number')
      expect(generatedConfig.allowReview).toBeTypeOf('boolean')
      
      // Verify question structure
      const codeQuestion = generatedConfig.questions[0]
      expect(codeQuestion.id).toBeDefined()
      expect(codeQuestion.type).toBe('code')
      expect(codeQuestion.question).toBeDefined()
      expect(codeQuestion.points).toBeTypeOf('number')
      
      const explanationQuestion = generatedConfig.questions[1]
      expect(explanationQuestion.type).toBe('explanation')
      expect(explanationQuestion.minWords).toBeTypeOf('number')
    })
  })

  describe('File System Integration', () => {
    it('should generate katas that integrate with existing kata loading system', async () => {
      // Mock generated kata content
      const generatedContent: GeneratedKataContent = {
        metadata: {
          slug: 'ai-generated-test',
          title: 'AI Generated Test Kata',
          language: 'py',
          type: 'code',
          difficulty: 'medium',
          tags: ['algorithms', 'ai-generated'],
          entry: 'entry.py',
          test: { kind: 'programmatic', file: 'tests.py' },
          timeout_ms: 5000
        },
        statement: '# AI Generated Test Kata\n\nSolve this problem...',
        starterCode: 'def solution():\n    pass',
        testCode: 'def test_solution():\n    assert solution() == expected',
        solutionCode: 'def solution():\n    return "correct"'
      }

      // Mock the saveGeneratedKata method
      vi.spyOn(aiAuthoringService, 'saveGeneratedKata')
        .mockResolvedValue({
          success: true,
          path: '/path/to/katas/ai-generated-test',
          slug: 'ai-generated-test',
          files: ['meta.yaml', 'statement.md', 'entry.py', 'tests.py', 'solution.py'],
          errors: []
        })

      const result = await aiAuthoringService.saveGeneratedKata(generatedContent)

      expect(result.success).toBe(true)
      expect(result.slug).toBe('ai-generated-test')
      expect(result.files).toContain('meta.yaml')
      expect(result.files).toContain('statement.md')
      expect(result.files).toContain('entry.py')
      expect(result.files).toContain('tests.py')
    })
  })
})